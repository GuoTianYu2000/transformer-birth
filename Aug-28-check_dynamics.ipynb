{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "import logging\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import sys\n",
    "import yaml\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from probe_utils import *\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Optional, Tuple\n",
    "import os\n",
    "# os.chdir(\"/data/tianyu_guo/birth\")\n",
    "from data import DataArgs, Dataset, iterate_batches, make_dataset\n",
    "from ihead_full_model import ModelArgs, Transformer, forward_hook, test_value, test_sink\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path_server = \"/data/tianyu/birth/gens/pre-iclr/dynamics/dormant_copy_k3_bos1\"\n",
    "# run_path_server2=\"/data/tianyu_guo/birth/gens/special/dormant_copy_2\"\n",
    "model, cfg, x, y, ds = load_model(run_path_local=\"/Users/guotianyu/GitHub/birth/gens/special/markov\", run_path_server=run_path_server, n_layers=3, n_heads=1, bos_num=1, train_steps=10000, delim=0, with_data=True, data_path_local=\"/Users/guotianyu/GitHub/birth/data\", data_path_server=\"/data/tianyu/birth/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = forward_hook([], '')\n",
    "pred, outputs_list = model.modified_forward_with_hook(x, hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([140.7036,  27.3864,  17.4664,  27.2720,  32.0737,  31.4242,  31.6992,\n",
       "         16.8821,  32.4395,  32.2114], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_list[0]['output'].norm(dim=-1)[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(pred.flatten(0, 1), y.flatten(0, 1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the gradients of the model's parameters\n",
    "gradients = {}\n",
    "for name, param in model.named_parameters():\n",
    "    gradients[name] = param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tok_embeddings.weight', 'pos_embeddings.weight', 'layers.0.attention.wq.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wv.weight', 'layers.0.attention.wo.weight', 'layers.0.ff.w1.weight', 'layers.0.ff.w2.weight', 'layers.0.attention_norm.weight', 'layers.0.attention_norm.bias', 'layers.0.ff_norm.weight', 'layers.0.ff_norm.bias', 'layers.1.attention.wq.weight', 'layers.1.attention.wk.weight', 'layers.1.attention.wv.weight', 'layers.1.attention.wo.weight', 'layers.1.ff.w1.weight', 'layers.1.ff.w2.weight', 'layers.1.attention_norm.weight', 'layers.1.attention_norm.bias', 'layers.1.ff_norm.weight', 'layers.1.ff_norm.bias', 'layers.2.attention.wq.weight', 'layers.2.attention.wk.weight', 'layers.2.attention.wv.weight', 'layers.2.attention.wo.weight', 'layers.2.ff.w1.weight', 'layers.2.ff.w2.weight', 'layers.2.attention_norm.weight', 'layers.2.attention_norm.bias', 'layers.2.ff_norm.weight', 'layers.2.ff_norm.bias', 'norm.weight', 'norm.bias', 'output.weight'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients['layers.1.attention.wq.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
